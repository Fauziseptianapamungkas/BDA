{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d24defc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('iris.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Preprocessing data\n",
    "X = data.drop('species', axis=1)\n",
    "y = data['species']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1478c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the environment (not strictly necessary for RL in this case, but can conceptualize as states)\n",
    "class IrisEnvironment:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.actions = np.unique(y_train)\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.choice(self.actions)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        return action == self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aa71e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.q_table = pd.DataFrame(0, index=np.unique(env.y_train),\n",
    "                                    columns=np.unique(env.y_train))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice(self.q_table.columns)  # explore\n",
    "        else:\n",
    "            return self.q_table.loc[state].idxmax()  # exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        predict = self.q_table.loc[state, action]\n",
    "        target = reward + self.gamma * self.q_table.loc[next_state].max()\n",
    "        self.q_table.loc[state, action] += self.alpha * (target - predict)\n",
    "\n",
    "    def train(self, episodes):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                reward = self.env.step(action)\n",
    "                next_state = action if reward else state\n",
    "                self.learn(state, action, reward, next_state)\n",
    "                state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a3890ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.q_table = pd.DataFrame(0, index=np.unique(env.y_train),\n",
    "                                    columns=np.unique(env.y_train))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice(self.q_table.columns)  # explore\n",
    "        else:\n",
    "            return self.q_table.loc[state].idxmax()  # exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        predict = self.q_table.loc[state, action]\n",
    "        target = reward + self.gamma * self.q_table.loc[next_state].max()\n",
    "        self.q_table.loc[state, action] += self.alpha * (target - predict)\n",
    "\n",
    "    def train(self, episodes):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                reward = self.env.step(action)\n",
    "                next_state = action if reward else state\n",
    "                self.learn(state, action, reward, next_state)\n",
    "                state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a9630d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display the Q-table\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ-Table:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(agent\u001b[38;5;241m.\u001b[39mq_table)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Visualize Q-Table as heatmap\u001b[39;00m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display the Q-table\n",
    "print(\"Q-Table:\")\n",
    "print(agent.q_table)\n",
    "\n",
    "# Visualize Q-Table as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(agent.q_table, annot=True, cmap=\"YlGnBu\", cbar=False)\n",
    "plt.title('Q-Table Heatmap')\n",
    "plt.xlabel('Actions')\n",
    "plt.ylabel('States')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b80fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
